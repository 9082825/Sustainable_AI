{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afcede0",
   "metadata": {},
   "source": [
    "# Sustainable AI (Streamlit)\n",
    "\n",
    "This notebook generates a **Streamlit web app** that lets a user:\n",
    "\n",
    "- Enter a **prompt** and model configuration\n",
    "- See **predicted energy consumption (kWh)**\n",
    "- Get an **alternative prompt suggestion** (shorter / lower-energy)\n",
    "- Compare energy for original vs alternative prompt\n",
    "\n",
    "The notebook will create an `app.py` file with Streamlit code that you can run via:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849357c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Cell 2 ‚Äì Install dependencies\n",
    "\n",
    "If your environment already has Streamlit, joblib, etc., you can skip this.\n",
    "\n",
    "```python\n",
    "!pip install streamlit joblib numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c599cab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import joblib\n",
    "from typing import Optional\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Model loading / prediction helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "@st.cache_resource\n",
    "def load_energy_model(path: str = \"model/energy_predictor/energy_predictor.pkl\"):\n",
    "    \"\"\"\n",
    "    Try to load a trained energy prediction model.\n",
    "    If not available, returns None and a simple heuristic will be used.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = joblib.load(path)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        # We keep this as a warning so the app still runs.\n",
    "        st.warning(\n",
    "            \"‚ö†Ô∏è Could not load trained energy model from \"\n",
    "            f\"`{path}`. Falling back to a simple heuristic.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "def estimate_prompt_tokens(prompt: str) -> int:\n",
    "    \"\"\"\n",
    "    Very rough token estimate: number of whitespace-separated words.\n",
    "    In a real system, you'd use the tokenizer of the target LLM.\n",
    "    \"\"\"\n",
    "    return max(1, len(prompt.split()))\n",
    "\n",
    "\n",
    "def predict_energy(\n",
    "    num_layers: int,\n",
    "    training_hours: float,\n",
    "    flops_per_hour: float,\n",
    "    prompt: str,\n",
    "    model=None,\n",
    "    default_gpu_power: float = 250.0,\n",
    "    default_batch_size: int = 32,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Predict energy consumption (kWh) for a given configuration.\n",
    "    - If a trained model is available, use it.\n",
    "    - Otherwise, use a simple heuristic based on layers, hours, FLOPs, and prompt length.\n",
    "    \"\"\"\n",
    "    tokens = estimate_prompt_tokens(prompt)\n",
    "\n",
    "    if model is not None:\n",
    "        # Adjust this feature order to match your trained model\n",
    "        X = np.array(\n",
    "            [[\n",
    "                num_layers,\n",
    "                training_hours,\n",
    "                flops_per_hour,\n",
    "                tokens,            # prompt_tokens\n",
    "                default_gpu_power, # gpu_power_watts\n",
    "                default_batch_size # batch_size\n",
    "            ]],\n",
    "            dtype=float,\n",
    "        )\n",
    "        return float(model.predict(X)[0])\n",
    "\n",
    "    # ----- Heuristic fallback (no trained model) -----\n",
    "    # Base term scaled into a reasonable range\n",
    "    base = num_layers * training_hours * flops_per_hour * 1e-19\n",
    "    complexity = np.log1p(tokens) / 10.0\n",
    "    energy = base * (0.5 + complexity)\n",
    "\n",
    "    # Clip to a sensible range\n",
    "    return float(np.clip(energy, 0.1, 5000.0))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Prompt simplification helper\n",
    "# (You can later replace this with your T5 / GPT-2 / OpenAI-based optimizer)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def simplify_prompt(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder simplifier:\n",
    "    - If the prompt is longer than 40 tokens, keep the first 40 and add '...'.\n",
    "    - Otherwise, return it as is.\n",
    "\n",
    "    Replace this later with a call to your:\n",
    "    - T5 model, or\n",
    "    - sentence-embedding-based optimizer, or\n",
    "    - OpenAI API.\n",
    "    \"\"\"\n",
    "    tokens = prompt.split()\n",
    "    if len(tokens) <= 40:\n",
    "        return prompt.strip()\n",
    "    return \" \".join(tokens[:40]) + \" ...\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Streamlit UI\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "        page_title=\"Sustainable AI ‚Äì Sustainable AI Prompt UI\",\n",
    "        page_icon=\"‚ö°\",\n",
    "        layout=\"centered\",\n",
    "    )\n",
    "\n",
    "    st.title(\"‚ö° Sustainable AI\")\n",
    "    st.caption(\"Part of the Sustainable AI project ‚Äì estimates energy and suggests lower-energy prompts.\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        ### How it works\n",
    "\n",
    "        1. Enter your **prompt** and basic **model configuration**.  \n",
    "        2. The app estimates **energy consumption (kWh)** for the given setup.  \n",
    "        3. It also proposes a **simpler alternative prompt** and shows its **estimated energy**.\n",
    "\n",
    "        > Note: This is a proof-of-concept. Estimates are based on a synthetic or approximated model, not on real hardware measurements.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # ---------------------- Input form ----------------------\n",
    "    with st.form(\"energy_form\"):\n",
    "        prompt = st.text_area(\n",
    "            \"üìù Prompt text\",\n",
    "            height=160,\n",
    "            placeholder=\"Describe what you want the model to do...\",\n",
    "        )\n",
    "\n",
    "        col1, col2 = st.columns(2)\n",
    "\n",
    "        with col1:\n",
    "            num_layers = st.number_input(\n",
    "                \"Number of model layers\",\n",
    "                min_value=1,\n",
    "                max_value=1000,\n",
    "                value=24,\n",
    "                step=1,\n",
    "            )\n",
    "\n",
    "            training_hours = st.number_input(\n",
    "                \"Training time (hours)\",\n",
    "                min_value=0.1,\n",
    "                max_value=2000.0,\n",
    "                value=8.0,\n",
    "                step=0.5,\n",
    "            )\n",
    "\n",
    "        with col2:\n",
    "            flops_per_hour = st.number_input(\n",
    "                \"Estimated FLOPs per hour\",\n",
    "                min_value=1e5,\n",
    "                max_value=1e21,\n",
    "                value=1e18,\n",
    "                step=1e16,\n",
    "                format=\"%.2e\",\n",
    "                help=\"Use scientific notation (e.g., 1e18).\",\n",
    "            )\n",
    "\n",
    "        submitted = st.form_submit_button(\"üöÄ Estimate energy\")\n",
    "\n",
    "    # ---------------------- Compute & display results ----------------------\n",
    "    if submitted:\n",
    "        if not prompt.strip():\n",
    "            st.error(\"Please enter a prompt before estimating energy.\")\n",
    "            return\n",
    "\n",
    "        # Load model (or heuristic)\n",
    "        model = load_energy_model()\n",
    "\n",
    "        # Baseline energy for original prompt\n",
    "        baseline_energy = predict_energy(\n",
    "            num_layers=num_layers,\n",
    "            training_hours=training_hours,\n",
    "            flops_per_hour=flops_per_hour,\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        # Alternative (simplified) prompt and its energy\n",
    "        alt_prompt = simplify_prompt(prompt)\n",
    "        alt_energy = predict_energy(\n",
    "            num_layers=num_layers,\n",
    "            training_hours=training_hours,\n",
    "            flops_per_hour=flops_per_hour,\n",
    "            prompt=alt_prompt,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        delta_energy = alt_energy - baseline_energy\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üîç Results\")\n",
    "\n",
    "        colA, colB = st.columns(2)\n",
    "\n",
    "        with colA:\n",
    "            st.metric(\n",
    "                label=\"Baseline energy (kWh)\",\n",
    "                value=f\"{baseline_energy:,.3f}\",\n",
    "            )\n",
    "            st.text_area(\n",
    "                \"Original prompt\",\n",
    "                value=prompt,\n",
    "                height=180,\n",
    "            )\n",
    "\n",
    "        with colB:\n",
    "            st.metric(\n",
    "                label=\"Alternative energy (kWh)\",\n",
    "                value=f\"{alt_energy:,.3f}\",\n",
    "                delta=f\"{delta_energy:,.3f} kWh\",\n",
    "                delta_color=\"inverse\" if delta_energy < 0 else \"normal\",\n",
    "            )\n",
    "            st.text_area(\n",
    "                \"Alternative prompt (suggested)\",\n",
    "                value=alt_prompt,\n",
    "                height=180,\n",
    "            )\n",
    "\n",
    "        st.caption(\n",
    "            \"Lower energy is better. Negative delta means the alternative prompt is estimated to use less energy.\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
