{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6358ad",
   "metadata": {},
   "source": [
    "#  Prompt Understanding & Optimization (Proof of Concept)\n",
    "\n",
    "This notebook:\n",
    "- Parses a user prompt\n",
    "- Assesses its linguistic complexity\n",
    "- Generates a more concise or semantically similar version\n",
    "- (Optionally) compares semantic similarity using sentence embeddings\n",
    "\n",
    "Future integration:\n",
    "- Use as a backend service for Sustainable AI Prompt Optimizer UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d6e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Conestoga Projects\\CSCN8010_MLF\\Sustainable_AI\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joseg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joseg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports & basic setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP / embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For simplification using T5 (can switch to GPT-2 or API later)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Linguistic processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure NLTK resources are ready\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67e9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt parsing & basic text utilities\n",
    "#Goal: clean text, tokenize, and get basic stats.\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def tokenize_words(text: str):\n",
    "    # simple word split; you can swap in nltk.word_tokenize if needed\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text)\n",
    "    return tokens\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    # very simple sentence splitter\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf81702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_count': 128,\n",
       " 'token_count': 22,\n",
       " 'unique_token_count': 22,\n",
       " 'type_token_ratio': 1.0,\n",
       " 'stopword_ratio': 0.45454545454545453,\n",
       " 'sentence_count': 1,\n",
       " 'avg_sentence_len': np.float64(22.0)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linguistic complexity metrics\n",
    "#We want: length, type–token ratio, stopword ratio, avg sentence length, etc.\n",
    "\n",
    "def compute_complexity_features(prompt: str) -> dict:\n",
    "    text = normalize_whitespace(prompt)\n",
    "    tokens = tokenize_words(text)\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    n_tokens = len(tokens)\n",
    "    n_types = len(set(tokens)) if tokens else 0\n",
    "    type_token_ratio = n_types / n_tokens if n_tokens > 0 else 0.0\n",
    "\n",
    "    n_stopwords = sum(1 for t in tokens if t in STOPWORDS)\n",
    "    stopword_ratio = n_stopwords / n_tokens if n_tokens > 0 else 0.0\n",
    "\n",
    "    avg_sentence_len = np.mean([len(tokenize_words(s)) for s in sentences]) if sentences else 0.0\n",
    "\n",
    "    features = {\n",
    "        \"char_count\": len(prompt),\n",
    "        \"token_count\": n_tokens,\n",
    "        \"unique_token_count\": n_types,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"avg_sentence_len\": avg_sentence_len,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# quick smoke test\n",
    "sample = \"Could you please help me rewrite this very long and slightly redundant prompt, so it becomes shorter but keeps the same meaning?\"\n",
    "compute_complexity_features(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befb51e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Conestoga Projects\\CSCN8010_MLF\\Sustainable_AI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\joseg\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "#Load a lightweight sentence transformer model\n",
    "#Use sentence-transformers for embeddings, e.g. all-MiniLM-L6-v2.\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    emb = embedder.encode([text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return emb[0]\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544a6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T5-based prompt simplifier\n",
    "#We’ll use a small T5 model (you can swap models later). Prompt pattern: \"summarize:\" or \"paraphrase:\".\n",
    "simplifier_model_name = \"t5-small\"  # can be upgraded later\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(simplifier_model_name)\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(simplifier_model_name)\n",
    "\n",
    "def simplify_prompt(prompt: str,\n",
    "                    max_length: int = 64,\n",
    "                    num_beams: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Uses a T5 model to generate a shorter / simplified version of the prompt.\n",
    "    \"\"\"\n",
    "    input_text = f\"summarize: {prompt}\"\n",
    "    inputs = t5_tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "    output_ids = t5_model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    simplified = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return simplified.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09acfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic similarity between original & simplified prompt\n",
    "#Now combine embeddings + T5 to check “did we keep the meaning?”\n",
    "\n",
    "def evaluate_simplification(original: str, simplified: str) -> dict:\n",
    "    # complexity before and after\n",
    "    orig_features = compute_complexity_features(original)\n",
    "    simp_features = compute_complexity_features(simplified)\n",
    "\n",
    "    # embeddings + cosine similarity\n",
    "    emb_orig = get_embedding(original)\n",
    "    emb_simp = get_embedding(simplified)\n",
    "    sim = cosine_sim(emb_orig, emb_simp)\n",
    "\n",
    "    return {\n",
    "        \"original\": original,\n",
    "        \"simplified\": simplified,\n",
    "        \"semantic_similarity\": sim,\n",
    "        \"original_features\": orig_features,\n",
    "        \"simplified_features\": simp_features,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982520e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End-to-end “optimize_prompt” pipeline\n",
    "#This function is what you’ll eventually call from a UI or API.\n",
    "def optimize_prompt(user_prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given a user prompt:\n",
    "    - Compute its complexity\n",
    "    - Suggest a simplified version\n",
    "    - Compute semantic similarity between original & simplified\n",
    "    \"\"\"\n",
    "    original = normalize_whitespace(user_prompt)\n",
    "\n",
    "    # 1) Complexity metrics\n",
    "    complexity = compute_complexity_features(original)\n",
    "\n",
    "    # 2) Simplified version\n",
    "    simplified = simplify_prompt(original)\n",
    "\n",
    "    # 3) Semantic similarity & delta in complexity\n",
    "    result = evaluate_simplification(original, simplified)\n",
    "\n",
    "    return {\n",
    "        \"complexity\": complexity,\n",
    "        \"simplified_prompt\": simplified,\n",
    "        \"semantic_similarity\": result[\"semantic_similarity\"],\n",
    "        \"simplified_complexity\": result[\"simplified_features\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26bffcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Prompt ===\n",
      "I would like you to please generate a detailed, step-by-step explanation of how to deploy a medium-sized transformer model to production, including all possible configuration options, even if some of them are not strictly necessary for a basic deployment.\n",
      "\n",
      "=== Simplified Prompt ===\n",
      "I would like you to generate a detailed, step-by-step explanation of how to deploy a medium-sized transformer model to production, including all possible configuration options. some of the options are not strictly necessary for a basic deployment.\n",
      "\n",
      "=== Semantic Similarity ===\n",
      "0.994\n",
      "\n",
      "=== Original Complexity ===\n",
      "char_count          : 255\n",
      "token_count         : 42\n",
      "unique_token_count  : 36\n",
      "type_token_ratio    : 0.8571428571428571\n",
      "stopword_ratio      : 0.4523809523809524\n",
      "sentence_count      : 1\n",
      "avg_sentence_len    : 42.0\n",
      "\n",
      "=== Simplified Complexity ===\n",
      "char_count          : 247\n",
      "token_count         : 40\n",
      "unique_token_count  : 33\n",
      "type_token_ratio    : 0.825\n",
      "stopword_ratio      : 0.45\n",
      "sentence_count      : 2\n",
      "avg_sentence_len    : 20.0\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "I would like you to please generate a detailed, step-by-step explanation of how to deploy\n",
    "a medium-sized transformer model to production, including all possible configuration options,\n",
    "even if some of them are not strictly necessary for a basic deployment.\n",
    "\"\"\"\n",
    "\n",
    "result = optimize_prompt(test_prompt)\n",
    "\n",
    "print(\"=== Original Prompt ===\")\n",
    "print(normalize_whitespace(test_prompt))\n",
    "print(\"\\n=== Simplified Prompt ===\")\n",
    "print(result[\"simplified_prompt\"])\n",
    "print(\"\\n=== Semantic Similarity ===\")\n",
    "print(f\"{result['semantic_similarity']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Original Complexity ===\")\n",
    "for k, v in result[\"complexity\"].items():\n",
    "    print(f\"{k:20s}: {v}\")\n",
    "\n",
    "print(\"\\n=== Simplified Complexity ===\")\n",
    "for k, v in result[\"simplified_complexity\"].items():\n",
    "    print(f\"{k:20s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b2f08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "ref": "efab5909-cf7a-482d-9b88-f14aa650c4d9",
       "rows": [
        [
         "0",
         "Explain gradient descent in simple terms.",
         "0"
        ],
        [
         "1",
         "Summarize the concept of backpropagation.",
         "0"
        ],
        [
         "2",
         "How do I cook pasta?",
         "1"
        ],
        [
         "3",
         "Give me tips to optimize a neural network.",
         "0"
        ],
        [
         "4",
         "Best way to boil spaghetti?",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain gradient descent in simple terms.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize the concept of backpropagation.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I cook pasta?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give me tips to optimize a neural network.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best way to boil spaghetti?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       prompt  cluster\n",
       "0   Explain gradient descent in simple terms.        0\n",
       "1   Summarize the concept of backpropagation.        0\n",
       "2                        How do I cook pasta?        1\n",
       "3  Give me tips to optimize a neural network.        0\n",
       "4                 Best way to boil spaghetti?        1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Batch evaluation & clustering\n",
    "#If we need to add the “clustering” part from our spec:\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_prompts(prompts, n_clusters=3):\n",
    "    embs = embedder.encode(prompts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = km.fit_predict(embs)\n",
    "    return labels, km\n",
    "\n",
    "# Example usage:\n",
    "prompt_list = [\n",
    "    \"Explain gradient descent in simple terms.\",\n",
    "    \"Summarize the concept of backpropagation.\",\n",
    "    \"How do I cook pasta?\",\n",
    "    \"Give me tips to optimize a neural network.\",\n",
    "    \"Best way to boil spaghetti?\"\n",
    "]\n",
    "\n",
    "labels, model_km = cluster_prompts(prompt_list, n_clusters=2)\n",
    "pd.DataFrame({\"prompt\": prompt_list, \"cluster\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b67170af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "original_prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "simplified_prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "semantic_similarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "orig_char_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "orig_token_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "orig_unique_token_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "orig_type_token_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "orig_stopword_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "orig_sentence_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "orig_avg_sentence_len",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "simp_char_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "simp_token_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "simp_unique_token_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "simp_type_token_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "simp_stopword_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "simp_sentence_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "simp_avg_sentence_len",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1d62914c-52ef-43ec-9c23-82a1b588bbfa",
       "rows": [
        [
         "0",
         "Please rewrite this extremely verbose and redundant prompt into something shorter.",
         "rewrite this extremely verbose and redundant prompt into something shorter.",
         "0.9658151865005493",
         "82",
         "11",
         "11",
         "1.0",
         "0.2727272727272727",
         "1",
         "11.0",
         "75",
         "10",
         "10",
         "1.0",
         "0.3",
         "1",
         "10.0"
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>simplified_prompt</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>orig_char_count</th>\n",
       "      <th>orig_token_count</th>\n",
       "      <th>orig_unique_token_count</th>\n",
       "      <th>orig_type_token_ratio</th>\n",
       "      <th>orig_stopword_ratio</th>\n",
       "      <th>orig_sentence_count</th>\n",
       "      <th>orig_avg_sentence_len</th>\n",
       "      <th>simp_char_count</th>\n",
       "      <th>simp_token_count</th>\n",
       "      <th>simp_unique_token_count</th>\n",
       "      <th>simp_type_token_ratio</th>\n",
       "      <th>simp_stopword_ratio</th>\n",
       "      <th>simp_sentence_count</th>\n",
       "      <th>simp_avg_sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please rewrite this extremely verbose and redu...</td>\n",
       "      <td>rewrite this extremely verbose and redundant p...</td>\n",
       "      <td>0.965815</td>\n",
       "      <td>82</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0  Please rewrite this extremely verbose and redu...   \n",
       "\n",
       "                                   simplified_prompt  semantic_similarity  \\\n",
       "0  rewrite this extremely verbose and redundant p...             0.965815   \n",
       "\n",
       "   orig_char_count  orig_token_count  orig_unique_token_count  \\\n",
       "0               82                11                       11   \n",
       "\n",
       "   orig_type_token_ratio  orig_stopword_ratio  orig_sentence_count  \\\n",
       "0                    1.0             0.272727                    1   \n",
       "\n",
       "   orig_avg_sentence_len  simp_char_count  simp_token_count  \\\n",
       "0                   11.0               75                10   \n",
       "\n",
       "   simp_unique_token_count  simp_type_token_ratio  simp_stopword_ratio  \\\n",
       "0                       10                    1.0                  0.3   \n",
       "\n",
       "   simp_sentence_count  simp_avg_sentence_len  \n",
       "0                    1                   10.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can save complexity results, or later export a small CSV of prompts before/after optimization.\n",
    "\n",
    "def log_optimization(user_prompt: str, path: str = \"prompt_optimization_log.csv\"):\n",
    "    res = optimize_prompt(user_prompt)\n",
    "    row = {\n",
    "        \"original_prompt\": normalize_whitespace(user_prompt),\n",
    "        \"simplified_prompt\": res[\"simplified_prompt\"],\n",
    "        \"semantic_similarity\": res[\"semantic_similarity\"],\n",
    "        **{f\"orig_{k}\": v for k, v in res[\"complexity\"].items()},\n",
    "        **{f\"simp_{k}\": v for k, v in res[\"simplified_complexity\"].items()},\n",
    "    }\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    try:\n",
    "        df_existing = pd.read_csv(path)\n",
    "        df = pd.concat([df_existing, df_row], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        df = df_row\n",
    "\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "# test log\n",
    "log_optimization(\"Please rewrite this extremely verbose and redundant prompt into something shorter.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
